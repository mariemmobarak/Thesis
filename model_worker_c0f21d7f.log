2025-03-23 04:24:41 | INFO | model_worker | args: Namespace(host='localhost', port=21002, worker_address='http://localhost:21002', controller_address='http://localhost:21001', model_path='lmsys/vicuna-7b-v1.5', revision='main', device='cuda', gpus=None, num_gpus=1, max_gpu_memory=None, dtype=None, load_8bit=False, cpu_offloading=False, gptq_ckpt=None, gptq_wbits=16, gptq_groupsize=-1, gptq_act_order=False, awq_ckpt=None, awq_wbits=16, awq_groupsize=-1, enable_exllama=False, exllama_max_seq_len=4096, exllama_gpu_split=None, exllama_cache_8bit=False, enable_xft=False, xft_max_seq_len=4096, xft_dtype=None, model_names=None, conv_template=None, embed_in_truncate=False, limit_worker_concurrency=5, stream_interval=2, no_register=False, seed=None, debug=False, ssl=False)
2025-03-23 04:24:41 | INFO | model_worker | Loading the model ['vicuna-7b-v1.5'] on worker c0f21d7f ...
2025-03-23 04:24:42 | ERROR | stderr | tokenizer_config.json:   0%|                                             | 0.00/749 [00:00<?, ?B/s]
2025-03-23 04:24:42 | ERROR | stderr | tokenizer_config.json: 100%|██████████████████████████████████████████████| 749/749 [00:00<?, ?B/s]
2025-03-23 04:24:42 | ERROR | stderr | 
2025-03-23 04:24:42 | ERROR | stderr | D:\UNI\Semester 8\Motzart\MozartsTouch\.venv\Lib\site-packages\huggingface_hub\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\Mariem Mobarak\.cache\huggingface\hub\models--lmsys--vicuna-7b-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
2025-03-23 04:24:42 | ERROR | stderr | To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
2025-03-23 04:24:42 | ERROR | stderr |   warnings.warn(message)
2025-03-23 04:24:42 | ERROR | stderr | Traceback (most recent call last):
2025-03-23 04:24:42 | ERROR | stderr |   File "<frozen runpy>", line 198, in _run_module_as_main
2025-03-23 04:24:42 | ERROR | stderr |   File "<frozen runpy>", line 88, in _run_code
2025-03-23 04:24:42 | ERROR | stderr |   File "D:\UNI\Semester 8\Motzart\MozartsTouch\.venv\Lib\site-packages\fastchat\serve\model_worker.py", line 414, in <module>
2025-03-23 04:24:42 | ERROR | stderr |     args, worker = create_model_worker()
2025-03-23 04:24:42 | ERROR | stderr |                    ^^^^^^^^^^^^^^^^^^^^^
2025-03-23 04:24:42 | ERROR | stderr |   File "D:\UNI\Semester 8\Motzart\MozartsTouch\.venv\Lib\site-packages\fastchat\serve\model_worker.py", line 385, in create_model_worker
2025-03-23 04:24:42 | ERROR | stderr |     worker = ModelWorker(
2025-03-23 04:24:42 | ERROR | stderr |              ^^^^^^^^^^^^
2025-03-23 04:24:42 | ERROR | stderr |   File "D:\UNI\Semester 8\Motzart\MozartsTouch\.venv\Lib\site-packages\fastchat\serve\model_worker.py", line 77, in __init__
2025-03-23 04:24:42 | ERROR | stderr |     self.model, self.tokenizer = load_model(
2025-03-23 04:24:42 | ERROR | stderr |                                  ^^^^^^^^^^^
2025-03-23 04:24:42 | ERROR | stderr |   File "D:\UNI\Semester 8\Motzart\MozartsTouch\.venv\Lib\site-packages\fastchat\model\model_adapter.py", line 353, in load_model
2025-03-23 04:24:42 | ERROR | stderr |     model, tokenizer = adapter.load_model(model_path, kwargs)
2025-03-23 04:24:42 | ERROR | stderr |                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-03-23 04:24:42 | ERROR | stderr |   File "D:\UNI\Semester 8\Motzart\MozartsTouch\.venv\Lib\site-packages\fastchat\model\model_adapter.py", line 686, in load_model
2025-03-23 04:24:42 | ERROR | stderr |     tokenizer = AutoTokenizer.from_pretrained(
2025-03-23 04:24:42 | ERROR | stderr |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-03-23 04:24:42 | ERROR | stderr |   File "D:\UNI\Semester 8\Motzart\MozartsTouch\.venv\Lib\site-packages\transformers\models\auto\tokenization_auto.py", line 973, in from_pretrained
2025-03-23 04:24:42 | ERROR | stderr |     return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
2025-03-23 04:24:42 | ERROR | stderr |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-03-23 04:24:42 | ERROR | stderr |   File "D:\UNI\Semester 8\Motzart\MozartsTouch\.venv\Lib\site-packages\transformers\utils\import_utils.py", line 1841, in __getattribute__
2025-03-23 04:24:42 | ERROR | stderr |     requires_backends(cls, cls._backends)
2025-03-23 04:24:42 | ERROR | stderr |   File "D:\UNI\Semester 8\Motzart\MozartsTouch\.venv\Lib\site-packages\transformers\utils\import_utils.py", line 1829, in requires_backends
2025-03-23 04:24:42 | ERROR | stderr |     raise ImportError("".join(failed))
2025-03-23 04:24:42 | ERROR | stderr | ImportError:
2025-03-23 04:24:42 | ERROR | stderr | LlamaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the
2025-03-23 04:24:42 | ERROR | stderr | installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones
2025-03-23 04:24:42 | ERROR | stderr | that match your environment. Please note that you may need to restart your runtime after installation.
2025-03-23 04:24:42 | ERROR | stderr | 
